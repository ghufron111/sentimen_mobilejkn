{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1821b8c-c599-402f-94c4-a98136e5d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ac447bd-bf7e-40f1-a165-7ea21b39965f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kurotsuki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kurotsuki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kurotsuki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function for preprocessing text\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('dataset/mobile_jkn.csv')  # Load your CSV file\n",
    "\n",
    "# Preprocess text data\n",
    "data['content'] = data['content'].apply(preprocess_text)\n",
    "\n",
    "# Remove empty content\n",
    "data = data[data['content'].str.strip() != '']  # Remove empty reviews\n",
    "\n",
    "# Features and labels\n",
    "X = data['content']\n",
    "y = data['score']\n",
    "\n",
    "# Count occurrences of each class\n",
    "class_counts = Counter(y)\n",
    "\n",
    "# Define target number of samples per class\n",
    "target_samples = 10000\n",
    "\n",
    "# List to store undersampled data\n",
    "undersampled_data = []\n",
    "\n",
    "# Loop through each class and adjust samples\n",
    "for label in class_counts.keys():\n",
    "    if class_counts[label] >= target_samples:\n",
    "        # Take a random sample of size target_samples\n",
    "        undersampled_data.append(data[data['score'] == label].sample(target_samples, random_state=42))\n",
    "    else:\n",
    "        # If the class has fewer samples than target_samples, replicate the data\n",
    "        class_data = data[data['score'] == label]\n",
    "        # Calculate how many times to replicate and get the remainder\n",
    "        times_to_replicate = target_samples // class_counts[label]\n",
    "        remainder = target_samples % class_counts[label]\n",
    "        \n",
    "        # Create the replicated dataset\n",
    "        replicated_data = pd.concat([class_data] * times_to_replicate + [class_data.sample(remainder, random_state=42)])\n",
    "        undersampled_data.append(replicated_data)\n",
    "\n",
    "# Concatenate all undersampled data\n",
    "undersampled_data = pd.concat(undersampled_data)\n",
    "\n",
    "# Features and labels after undersampling\n",
    "X = undersampled_data['content']\n",
    "y = undersampled_data['score']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "847e6752-8777-4328-8d4e-a6a9bb4b8d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undersampled y_train distribution: Counter({5: 7524, 4: 7523, 3: 7510, 2: 7494, 1: 7449})\n",
      "Resampled y_train distribution: Counter({3: 7524, 4: 7524, 0: 7524, 2: 7524, 1: 7524})\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for text data\n",
    "max_words = 10000\n",
    "max_len = 150\n",
    "embedding_dim = 128\n",
    "\n",
    "# Tokenization and Padding\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Label Binarization\n",
    "lb = LabelBinarizer()\n",
    "y_train_onehot = lb.fit_transform(y_train)\n",
    "y_test_onehot = lb.transform(y_test)\n",
    "\n",
    "# Check distribution after undersampling\n",
    "print(f\"Undersampled y_train distribution: {Counter(y_train)}\")\n",
    "\n",
    "# Ensure y_train_onehot is compatible with SMOTE\n",
    "if len(set(y_train_onehot.flatten())) < 2:\n",
    "    print(\"Not enough classes for SMOTE.\")\n",
    "else:\n",
    "    # Resampling using SMOTE\n",
    "    smote = SMOTE(sampling_strategy='auto', random_state=42)  # Generates a balance\n",
    "    X_train_resampled, y_train_onehot_resampled = smote.fit_resample(X_train_padded, y_train_onehot)\n",
    "\n",
    "# Flattening for compatibility with models\n",
    "y_train_flat = y_train_onehot_resampled.argmax(axis=1)\n",
    "y_test_flat = y_test_onehot.argmax(axis=1)\n",
    "\n",
    "# Debugging: Check class distribution\n",
    "print(f\"Resampled y_train distribution: {Counter(y_train_flat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20deb890-4553-4c76-a0c3-b286acf71ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "294/294 [==============================] - 13s 36ms/step - loss: 1.8243 - accuracy: 0.3448 - val_loss: 1.2526 - val_accuracy: 0.4042\n",
      "Epoch 2/200\n",
      "294/294 [==============================] - 10s 34ms/step - loss: 1.2070 - accuracy: 0.4322 - val_loss: 1.1549 - val_accuracy: 0.4816\n",
      "Epoch 3/200\n",
      "294/294 [==============================] - 10s 33ms/step - loss: 1.1374 - accuracy: 0.5017 - val_loss: 1.1196 - val_accuracy: 0.5122\n",
      "Epoch 4/200\n",
      "294/294 [==============================] - 11s 37ms/step - loss: 1.0739 - accuracy: 0.5490 - val_loss: 1.0996 - val_accuracy: 0.5329\n",
      "Epoch 5/200\n",
      "294/294 [==============================] - 11s 39ms/step - loss: 1.0144 - accuracy: 0.5852 - val_loss: 1.0820 - val_accuracy: 0.5456\n",
      "Epoch 6/200\n",
      "294/294 [==============================] - 11s 37ms/step - loss: 0.9672 - accuracy: 0.6105 - val_loss: 1.0794 - val_accuracy: 0.5578\n",
      "Epoch 7/200\n",
      "294/294 [==============================] - 12s 41ms/step - loss: 0.9614 - accuracy: 0.6139 - val_loss: 1.0752 - val_accuracy: 0.5628\n",
      "Epoch 8/200\n",
      "294/294 [==============================] - 12s 41ms/step - loss: 0.9168 - accuracy: 0.6375 - val_loss: 1.0757 - val_accuracy: 0.5722\n",
      "Epoch 9/200\n",
      "294/294 [==============================] - 11s 38ms/step - loss: 0.8592 - accuracy: 0.6610 - val_loss: 1.0536 - val_accuracy: 0.5771\n",
      "Epoch 10/200\n",
      "294/294 [==============================] - 12s 40ms/step - loss: 0.8871 - accuracy: 0.6554 - val_loss: 1.0699 - val_accuracy: 0.5818\n",
      "Epoch 11/200\n",
      "294/294 [==============================] - 11s 39ms/step - loss: 0.8902 - accuracy: 0.6548 - val_loss: 1.0948 - val_accuracy: 0.5774\n",
      "Epoch 12/200\n",
      "294/294 [==============================] - 11s 38ms/step - loss: 0.8601 - accuracy: 0.6696 - val_loss: 1.0855 - val_accuracy: 0.5822\n",
      "Epoch 13/200\n",
      "294/294 [==============================] - 11s 38ms/step - loss: 0.8492 - accuracy: 0.6760 - val_loss: 1.0722 - val_accuracy: 0.5949\n",
      "Epoch 14/200\n",
      "294/294 [==============================] - 11s 37ms/step - loss: 0.8600 - accuracy: 0.6831 - val_loss: 1.1084 - val_accuracy: 0.5921\n",
      "Epoch 15/200\n",
      "294/294 [==============================] - 11s 37ms/step - loss: 0.8220 - accuracy: 0.6942 - val_loss: 1.0819 - val_accuracy: 0.5945\n",
      "Epoch 16/200\n",
      "294/294 [==============================] - 12s 41ms/step - loss: 0.7791 - accuracy: 0.7003 - val_loss: 1.0517 - val_accuracy: 0.6085\n",
      "Epoch 17/200\n",
      "294/294 [==============================] - 11s 37ms/step - loss: 0.7523 - accuracy: 0.7052 - val_loss: 1.0857 - val_accuracy: 0.6022\n",
      "Epoch 18/200\n",
      "294/294 [==============================] - 11s 38ms/step - loss: 0.7786 - accuracy: 0.7002 - val_loss: 1.0680 - val_accuracy: 0.6140\n",
      "Epoch 19/200\n",
      "294/294 [==============================] - 11s 39ms/step - loss: 0.8084 - accuracy: 0.7060 - val_loss: 1.0908 - val_accuracy: 0.6182\n",
      "Epoch 20/200\n",
      "294/294 [==============================] - 11s 37ms/step - loss: 0.7695 - accuracy: 0.7130 - val_loss: 1.0804 - val_accuracy: 0.6169\n",
      "Epoch 21/200\n",
      "294/294 [==============================] - 11s 39ms/step - loss: 0.7325 - accuracy: 0.7231 - val_loss: 1.0739 - val_accuracy: 0.6187\n",
      "Epoch 22/200\n",
      "294/294 [==============================] - 12s 40ms/step - loss: 0.7094 - accuracy: 0.7278 - val_loss: 1.0457 - val_accuracy: 0.6274\n",
      "Epoch 23/200\n",
      "294/294 [==============================] - 11s 38ms/step - loss: 0.6905 - accuracy: 0.7320 - val_loss: 1.0575 - val_accuracy: 0.6307\n",
      "Epoch 24/200\n",
      "294/294 [==============================] - 11s 39ms/step - loss: 0.6774 - accuracy: 0.7323 - val_loss: 1.0478 - val_accuracy: 0.6310\n",
      "Epoch 25/200\n",
      "294/294 [==============================] - 11s 39ms/step - loss: 0.6575 - accuracy: 0.7386 - val_loss: 1.0460 - val_accuracy: 0.6342\n",
      "Epoch 26/200\n",
      "294/294 [==============================] - 11s 38ms/step - loss: 0.6493 - accuracy: 0.7429 - val_loss: 1.0518 - val_accuracy: 0.6314\n",
      "Epoch 27/200\n",
      "294/294 [==============================] - 12s 42ms/step - loss: 0.6418 - accuracy: 0.7469 - val_loss: 1.0823 - val_accuracy: 0.6340\n",
      "Epoch 28/200\n",
      "294/294 [==============================] - 12s 39ms/step - loss: 0.6334 - accuracy: 0.7476 - val_loss: 1.0527 - val_accuracy: 0.6265\n",
      "Epoch 29/200\n",
      "294/294 [==============================] - 12s 41ms/step - loss: 0.6395 - accuracy: 0.7470 - val_loss: 1.0368 - val_accuracy: 0.6442\n",
      "Epoch 30/200\n",
      "294/294 [==============================] - 12s 40ms/step - loss: 0.6116 - accuracy: 0.7563 - val_loss: 1.0870 - val_accuracy: 0.6409\n",
      "Epoch 31/200\n",
      "294/294 [==============================] - 11s 39ms/step - loss: 0.6101 - accuracy: 0.7558 - val_loss: 1.0403 - val_accuracy: 0.6450\n",
      "Epoch 32/200\n",
      "294/294 [==============================] - 12s 41ms/step - loss: 0.6116 - accuracy: 0.7578 - val_loss: 1.0793 - val_accuracy: 0.6418\n",
      "Epoch 33/200\n",
      "294/294 [==============================] - 12s 41ms/step - loss: 0.5922 - accuracy: 0.7625 - val_loss: 1.0649 - val_accuracy: 0.6506\n",
      "Epoch 34/200\n",
      "294/294 [==============================] - 11s 38ms/step - loss: 0.5902 - accuracy: 0.7647 - val_loss: 1.1142 - val_accuracy: 0.6428\n",
      "Epoch 35/200\n",
      "294/294 [==============================] - 12s 40ms/step - loss: 0.7786 - accuracy: 0.7355 - val_loss: 1.2475 - val_accuracy: 0.6333\n",
      "Epoch 36/200\n",
      "294/294 [==============================] - 11s 39ms/step - loss: 0.7135 - accuracy: 0.7484 - val_loss: 1.1762 - val_accuracy: 0.6371\n",
      "Epoch 37/200\n",
      "294/294 [==============================] - 12s 39ms/step - loss: 0.6833 - accuracy: 0.7532 - val_loss: 1.1682 - val_accuracy: 0.6491\n",
      "Epoch 38/200\n",
      "294/294 [==============================] - 12s 42ms/step - loss: 0.6573 - accuracy: 0.7584 - val_loss: 1.1846 - val_accuracy: 0.6438\n",
      "Epoch 39/200\n",
      "294/294 [==============================] - 11s 38ms/step - loss: 0.6464 - accuracy: 0.7614 - val_loss: 1.1225 - val_accuracy: 0.6435\n",
      "Epoch 40/200\n",
      "294/294 [==============================] - 12s 40ms/step - loss: 0.6254 - accuracy: 0.7686 - val_loss: 1.1377 - val_accuracy: 0.6490\n",
      "Epoch 41/200\n",
      "294/294 [==============================] - 11s 39ms/step - loss: 0.6267 - accuracy: 0.7642 - val_loss: 1.1600 - val_accuracy: 0.6460\n",
      "Epoch 42/200\n",
      "294/294 [==============================] - 11s 38ms/step - loss: 0.6077 - accuracy: 0.7688 - val_loss: 1.1818 - val_accuracy: 0.6506\n",
      "Epoch 43/200\n",
      "294/294 [==============================] - 12s 40ms/step - loss: 0.5996 - accuracy: 0.7734 - val_loss: 1.1866 - val_accuracy: 0.6545\n",
      "Epoch 44/200\n",
      "294/294 [==============================] - 12s 41ms/step - loss: 0.5921 - accuracy: 0.7733 - val_loss: 1.1386 - val_accuracy: 0.6610\n",
      "Epoch 45/200\n",
      "294/294 [==============================] - 12s 39ms/step - loss: 0.5840 - accuracy: 0.7761 - val_loss: 1.1325 - val_accuracy: 0.6606\n",
      "Epoch 46/200\n",
      "294/294 [==============================] - 11s 39ms/step - loss: 0.5726 - accuracy: 0.7772 - val_loss: 1.1321 - val_accuracy: 0.6558\n",
      "Epoch 47/200\n",
      "294/294 [==============================] - 11s 39ms/step - loss: 0.5728 - accuracy: 0.7783 - val_loss: 1.1576 - val_accuracy: 0.6615\n",
      "Epoch 48/200\n",
      "294/294 [==============================] - 12s 40ms/step - loss: 0.5601 - accuracy: 0.7815 - val_loss: 1.1315 - val_accuracy: 0.6603\n",
      "Epoch 49/200\n",
      "294/294 [==============================] - 12s 41ms/step - loss: 0.5532 - accuracy: 0.7837 - val_loss: 1.2081 - val_accuracy: 0.6638\n",
      "Epoch 50/200\n",
      "294/294 [==============================] - 12s 40ms/step - loss: 0.5503 - accuracy: 0.7838 - val_loss: 1.1503 - val_accuracy: 0.6627\n",
      "Epoch 51/200\n",
      "294/294 [==============================] - 12s 39ms/step - loss: 0.5431 - accuracy: 0.7871 - val_loss: 1.1907 - val_accuracy: 0.6661\n",
      "Epoch 52/200\n",
      "294/294 [==============================] - 11s 39ms/step - loss: 0.5555 - accuracy: 0.7825 - val_loss: 1.1661 - val_accuracy: 0.6644\n",
      "Epoch 53/200\n",
      "294/294 [==============================] - 12s 39ms/step - loss: 0.5489 - accuracy: 0.7843 - val_loss: 1.1797 - val_accuracy: 0.6671\n",
      "Epoch 54/200\n",
      "294/294 [==============================] - 11s 36ms/step - loss: 0.5367 - accuracy: 0.7881 - val_loss: 1.1858 - val_accuracy: 0.6702\n",
      "Epoch 55/200\n",
      "294/294 [==============================] - 11s 38ms/step - loss: 0.5320 - accuracy: 0.7896 - val_loss: 1.2696 - val_accuracy: 0.6660\n",
      "Epoch 56/200\n",
      "294/294 [==============================] - 11s 38ms/step - loss: 0.5365 - accuracy: 0.7877 - val_loss: 1.2362 - val_accuracy: 0.6634\n",
      "Epoch 57/200\n",
      "294/294 [==============================] - 11s 36ms/step - loss: 0.5281 - accuracy: 0.7908 - val_loss: 1.3188 - val_accuracy: 0.6555\n",
      "Epoch 58/200\n",
      "294/294 [==============================] - 13s 44ms/step - loss: 0.5283 - accuracy: 0.7907 - val_loss: 1.2237 - val_accuracy: 0.6702\n",
      "Epoch 59/200\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.5085 - accuracy: 0.7959 - val_loss: 1.2246 - val_accuracy: 0.6685\n",
      "Epoch 60/200\n",
      "294/294 [==============================] - 13s 46ms/step - loss: 0.5111 - accuracy: 0.7951 - val_loss: 1.3022 - val_accuracy: 0.6676\n",
      "Epoch 61/200\n",
      "294/294 [==============================] - 12s 42ms/step - loss: 0.5194 - accuracy: 0.7947 - val_loss: 1.2490 - val_accuracy: 0.6701\n",
      "Epoch 62/200\n",
      "294/294 [==============================] - 12s 40ms/step - loss: 0.5190 - accuracy: 0.7937 - val_loss: 1.1987 - val_accuracy: 0.6727\n",
      "Epoch 63/200\n",
      "294/294 [==============================] - 13s 43ms/step - loss: 0.5161 - accuracy: 0.7964 - val_loss: 1.2161 - val_accuracy: 0.6760\n",
      "Epoch 64/200\n",
      "294/294 [==============================] - 12s 41ms/step - loss: 0.5037 - accuracy: 0.7993 - val_loss: 1.1872 - val_accuracy: 0.6706\n",
      "Epoch 65/200\n",
      "294/294 [==============================] - 13s 43ms/step - loss: 0.5075 - accuracy: 0.7982 - val_loss: 1.2463 - val_accuracy: 0.6719\n",
      "Epoch 66/200\n",
      "294/294 [==============================] - 13s 43ms/step - loss: 0.4923 - accuracy: 0.8031 - val_loss: 1.2177 - val_accuracy: 0.6798\n",
      "Epoch 67/200\n",
      "294/294 [==============================] - 12s 40ms/step - loss: 0.5187 - accuracy: 0.7960 - val_loss: 1.2086 - val_accuracy: 0.6779\n",
      "Epoch 68/200\n",
      "294/294 [==============================] - 12s 41ms/step - loss: 0.5060 - accuracy: 0.7998 - val_loss: 1.2064 - val_accuracy: 0.6831\n",
      "Epoch 69/200\n",
      "294/294 [==============================] - 12s 41ms/step - loss: 0.4909 - accuracy: 0.8038 - val_loss: 1.2598 - val_accuracy: 0.6776\n",
      "Epoch 70/200\n",
      "294/294 [==============================] - 12s 41ms/step - loss: 0.5261 - accuracy: 0.7948 - val_loss: 1.2683 - val_accuracy: 0.6703\n",
      "Epoch 71/200\n",
      "294/294 [==============================] - 13s 44ms/step - loss: 0.5150 - accuracy: 0.7991 - val_loss: 1.2259 - val_accuracy: 0.6774\n",
      "Epoch 72/200\n",
      "294/294 [==============================] - 12s 41ms/step - loss: 0.4944 - accuracy: 0.8040 - val_loss: 1.3285 - val_accuracy: 0.6722\n",
      "Epoch 73/200\n",
      "294/294 [==============================] - 12s 42ms/step - loss: 0.4993 - accuracy: 0.7993 - val_loss: 1.2817 - val_accuracy: 0.6770\n",
      "Epoch 74/200\n",
      "294/294 [==============================] - 11s 36ms/step - loss: 0.4804 - accuracy: 0.8067 - val_loss: 1.2784 - val_accuracy: 0.6770\n",
      "Epoch 75/200\n",
      "294/294 [==============================] - 11s 36ms/step - loss: 0.4928 - accuracy: 0.8030 - val_loss: 1.1881 - val_accuracy: 0.6770\n",
      "Epoch 76/200\n",
      "294/294 [==============================] - 12s 40ms/step - loss: 0.4896 - accuracy: 0.8054 - val_loss: 1.2645 - val_accuracy: 0.6693\n",
      "Epoch 77/200\n",
      "294/294 [==============================] - 11s 36ms/step - loss: 0.5127 - accuracy: 0.8005 - val_loss: 1.3011 - val_accuracy: 0.6640\n",
      "Epoch 78/200\n",
      "294/294 [==============================] - 11s 37ms/step - loss: 0.5166 - accuracy: 0.7998 - val_loss: 1.2431 - val_accuracy: 0.6738\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 1.2064 - accuracy: 0.6831\n",
      "391/391 [==============================] - 5s 11ms/step\n",
      "LSTM Test Accuracy: 0.6831200122833252\n",
      "LSTM Precision: 0.6872163815115722\n",
      "LSTM Recall: 0.68312\n",
      "LSTM F1-Score: 0.6767465487450424\n"
     ]
    }
   ],
   "source": [
    "# --- Define and Train the LSTM Model ---\n",
    "lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(max_words, embedding_dim, input_length=max_len),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.01))),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(y_train_onehot.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "lstm_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the LSTM model\n",
    "lstm_model.fit(X_train_resampled, y_train_onehot_resampled, epochs=200, batch_size=128,\n",
    "               validation_data=(X_test_padded, y_test_onehot), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the LSTM model\n",
    "lstm_test_loss, lstm_test_accuracy = lstm_model.evaluate(X_test_padded, y_test_onehot)\n",
    "\n",
    "# Predict with the LSTM model\n",
    "y_pred_lstm = lstm_model.predict(X_test_padded).argmax(axis=1)\n",
    "\n",
    "# LSTM metrics\n",
    "lstm_precision = precision_score(y_test_flat, y_pred_lstm, average='weighted')\n",
    "lstm_recall = recall_score(y_test_flat, y_pred_lstm, average='weighted')\n",
    "lstm_f1 = f1_score(y_test_flat, y_pred_lstm, average='weighted')\n",
    "\n",
    "print(f\"LSTM Test Accuracy: {lstm_test_accuracy}\")\n",
    "print(f\"LSTM Precision: {lstm_precision}\")\n",
    "print(f\"LSTM Recall: {lstm_recall}\")\n",
    "print(f\"LSTM F1-Score: {lstm_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0567ea0-8458-4f8f-8928-bef17acfffa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train_padded: 37500\n",
      "Length of y_train_flat: 37620\n",
      "Mismatch in number of samples between X_train and y_train.\n",
      "Number of empty reviews in X_train before padding: 0\n",
      "Number of empty reviews in y_train: 0\n",
      "Length of X_train_padded after cleaning: 37500\n",
      "Length of y_train_flat after cleaning: 37500\n"
     ]
    }
   ],
   "source": [
    "# Check the length of features and labels\n",
    "print(f\"Length of X_train_padded: {len(X_train_padded)}\")\n",
    "print(f\"Length of y_train_flat: {len(y_train_flat)}\")\n",
    "\n",
    "# Ensure they have the same number of samples\n",
    "if len(X_train_padded) == len(y_train_flat):\n",
    "    # Apply SMOTE if sizes are consistent\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_padded, y_train_flat)\n",
    "    print(f\"Resampled y_train distribution: {Counter(y_train_resampled)}\")\n",
    "else:\n",
    "    print(\"Mismatch in number of samples between X_train and y_train.\")\n",
    "\n",
    "# Check for empty or invalid content in X_train before tokenization\n",
    "print(f\"Number of empty reviews in X_train before padding: {sum(X_train.str.strip() == '')}\")\n",
    "print(f\"Number of empty reviews in y_train: {sum(pd.isnull(y_train))}\")\n",
    "\n",
    "# Remove any rows where content is empty or y_train is NaN\n",
    "X_train_cleaned = X_train[X_train.str.strip() != '']\n",
    "y_train_cleaned = y_train[X_train.str.strip() != '']\n",
    "\n",
    "# Tokenization and Padding\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_cleaned)\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Binarize labels again after cleaning\n",
    "y_train_flat = lb.fit_transform(y_train_cleaned).argmax(axis=1)\n",
    "\n",
    "# Check the new lengths\n",
    "print(f\"Length of X_train_padded after cleaning: {len(X_train_padded)}\")\n",
    "print(f\"Length of y_train_flat after cleaning: {len(y_train_flat)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19773a06-fe80-4d5f-bbac-72e22246e581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Results:\n",
      "Accuracy: 0.28912\n",
      "Precision: 0.2863017159168845\n",
      "Recall: 0.28912\n",
      "F1 Score: 0.22578833413438085\n",
      "\n",
      "XGBoost Results:\n",
      "Accuracy: 0.58704\n",
      "Precision: 0.5855836347963148\n",
      "Recall: 0.58704\n",
      "F1 Score: 0.5797484927850259\n",
      "\n",
      "Random Forest Results:\n",
      "Accuracy: 0.74856\n",
      "Precision: 0.7518853889362966\n",
      "Recall: 0.74856\n",
      "F1 Score: 0.746661752159862\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# Apply SMOTE to handle imbalanced data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_padded, y_train_flat)\n",
    "\n",
    "# Define function to evaluate model\n",
    "def evaluate_model(model, X_test_padded, y_test_flat):\n",
    "    y_pred = model.predict(X_test_padded)\n",
    "    accuracy = accuracy_score(y_test_flat, y_pred)\n",
    "    precision = precision_score(y_test_flat, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test_flat, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test_flat, y_pred, average='weighted')\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Naive Bayes Model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_resampled, y_train_resampled)\n",
    "print(\"Naive Bayes Results:\")\n",
    "evaluate_model(nb_model, X_test_padded, y_test_flat)\n",
    "\n",
    "# XGBoost Model\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "print(\"\\nXGBoost Results:\")\n",
    "evaluate_model(xgb_model, X_test_padded, y_test_flat)\n",
    "\n",
    "# Random Forest Model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "print(\"\\nRandom Forest Results:\")\n",
    "evaluate_model(rf_model, X_test_padded, y_test_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf58f742-cef7-4c71-814e-5df101291fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray()\n",
    "\n",
    "# Reshape y_train agar sesuai dengan dimensi X_train_tfidf\n",
    "y_train_flat = y_train.values.flatten()  # pastikan y_train_flat adalah array 1D\n",
    "\n",
    "X_train_resampled, y_train_onehot_resampled = smote.fit_resample(X_train_tfidf, y_train_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b01febf2-8d23-4e8a-bbb7-3d103b9d48bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Test Accuracy: 0.05464\n",
      "RF Precision: 0.07311520675334408\n",
      "RF Recall: 0.05464\n",
      "RF F1-Score: 0.06163986004024331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kurotsuki\\PycharmProjects\\mobile_jkn_baru\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Kurotsuki\\PycharmProjects\\mobile_jkn_baru\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# --- Random Forest Model ---\n",
    "rf_model = RandomForestClassifier(random_state=128)\n",
    "rf_model.fit(X_train_resampled, y_train_onehot_resampled)\n",
    "\n",
    "# Predict with Random Forest\n",
    "y_pred_rf = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# Random Forest metrics\n",
    "rf_accuracy = accuracy_score(y_test_flat, y_pred_rf)\n",
    "rf_precision = precision_score(y_test_flat, y_pred_rf, average='weighted')\n",
    "rf_recall = recall_score(y_test_flat, y_pred_rf, average='weighted')\n",
    "rf_f1 = f1_score(y_test_flat, y_pred_rf, average='weighted')\n",
    "\n",
    "print(f\"RF Test Accuracy: {rf_accuracy}\")\n",
    "print(f\"RF Precision: {rf_precision}\")\n",
    "print(f\"RF Recall: {rf_recall}\")\n",
    "print(f\"RF F1-Score: {rf_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "498680af-e6a0-46e3-921c-096ef1539f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_resampled: (37620, 1000)\n",
      "Shape of y_resampled: (37620,)\n",
      "Accuracy: 0.6904\n",
      "Precision: 0.6951216888227025\n",
      "Recall: 0.6904\n",
      "F1 Score: 0.682103786261003\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray()\n",
    "\n",
    "# Reshape y_train agar sesuai dengan dimensi X_train_tfidf\n",
    "y_train_flat = y_train.values.flatten()  # pastikan y_train_flat adalah array 1D\n",
    "\n",
    "# Terapkan SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Lakukan resampling pada X_train_tfidf dan y_train_flat\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_tfidf, y_train_flat)\n",
    "\n",
    "# Lihat hasil setelah SMOTE\n",
    "print(f\"Shape of X_resampled: {X_resampled.shape}\")\n",
    "print(f\"Shape of y_resampled: {y_resampled.shape}\")\n",
    "\n",
    "# Proses pelatihan model\n",
    "# Misalnya menggunakan SVM atau model lain yang Anda pilih\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Prediksi dan evaluasi\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluasi model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d48fd1af-a409-4cef-aa59-b0f43c7b094f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.56928\n",
      "XGBoost Precision: 0.5671231562907131\n",
      "XGBoost Recall: 0.56928\n",
      "XGBoost F1 Score: 0.5613871805874229\n"
     ]
    }
   ],
   "source": [
    "# Normalisasi label agar mulai dari 0\n",
    "y_resampled_normalized = y_resampled - 1  # Jika kelas mulai dari 1 hingga 5\n",
    "\n",
    "# --------------------- XGBoost Model ---------------------\n",
    "xgb_model = XGBClassifier(eval_metric='mlogloss')  # Fine-tune hyperparameters di sini\n",
    "xgb_model.fit(X_resampled, y_resampled_normalized)  # Menggunakan label yang sudah dinormalisasi\n",
    "\n",
    "# Prediksi dengan XGBoost\n",
    "y_pred_xgb = xgb_model.predict(X_test_tfidf)\n",
    "\n",
    "# XGBoost Metrics\n",
    "xgb_accuracy = accuracy_score(y_test - 1, y_pred_xgb)  # Normalisasi y_test jika perlu\n",
    "xgb_precision = precision_score(y_test - 1, y_pred_xgb, average='weighted')\n",
    "xgb_recall = recall_score(y_test - 1, y_pred_xgb, average='weighted')\n",
    "xgb_f1 = f1_score(y_test - 1, y_pred_xgb, average='weighted')\n",
    "\n",
    "print(\"XGBoost Accuracy:\", xgb_accuracy)\n",
    "print(\"XGBoost Precision:\", xgb_precision)\n",
    "print(\"XGBoost Recall:\", xgb_recall)\n",
    "print(\"XGBoost F1 Score:\", xgb_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c2cdec4-74b0-4493-bc2c-8d6f4ed3cd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes Accuracy with SMOTE: 0.16816\n",
      "Naive Bayes Precision with SMOTE: 0.17953746186405525\n",
      "Naive Bayes Recall with SMOTE: 0.16816\n",
      "Naive Bayes F1 Score with SMOTE: 0.16810490673694825\n"
     ]
    }
   ],
   "source": [
    "# --------------------- Naive Bayes Model with SMOTE ---------------------\n",
    "nb_model = MultinomialNB(alpha=0.001)  # Adjust alpha for smoothing if necessary\n",
    "nb_model.fit(X_resampled, y_resampled)  # Menggunakan data hasil SMOTE\n",
    "\n",
    "# Predict with Naive Bayes\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "# Naive Bayes Metrics\n",
    "nb_accuracy = accuracy_score(y_test_flat, y_pred_nb)\n",
    "nb_precision = precision_score(y_test_flat, y_pred_nb, average='weighted', zero_division=0)\n",
    "nb_recall = recall_score(y_test_flat, y_pred_nb, average='weighted', zero_division=0)\n",
    "nb_f1 = f1_score(y_test_flat, y_pred_nb, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"\\nNaive Bayes Accuracy with SMOTE:\", nb_accuracy)\n",
    "print(\"Naive Bayes Precision with SMOTE:\", nb_precision)\n",
    "print(\"Naive Bayes Recall with SMOTE:\", nb_recall)\n",
    "print(\"Naive Bayes F1 Score with SMOTE:\", nb_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63f5a2-b2c8-4bed-b482-f8cb36cd4164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Results ---\n",
    "results = {\n",
    "    \"Model\": [\"LSTM\", \"Random Forest\", \"XGBoost\", \"Naive Bayes\"],\n",
    "    \"Accuracy\": [lstm_test_accuracy, rf_accuracy, xgb_accuracy, accuracy],\n",
    "    \"Precision\": [lstm_precision, rf_precision, xgb_precision, precision],\n",
    "    \"Recall\": [lstm_recall, rf_recall, xgb_recall, recall],\n",
    "    \"F1-Score\": [lstm_f1, rf_f1, xgb_f1, f1]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the DataFrame as a table\n",
    "print(\"\\nComparison of Model Results With SMOTE:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca712f2-df85-4edc-a879-f38e0065110b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
