{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1821b8c-c599-402f-94c4-a98136e5d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ac447bd-bf7e-40f1-a165-7ea21b39965f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kurotsuki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kurotsuki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kurotsuki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function for preprocessing text\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('dataset/mobile_jkn.csv')  # Load your CSV file\n",
    "\n",
    "# Preprocess text data\n",
    "data['content'] = data['content'].apply(preprocess_text)\n",
    "\n",
    "# Remove empty content\n",
    "data = data[data['content'].str.strip() != '']  # Remove empty reviews\n",
    "\n",
    "# Features and labels\n",
    "X = data['content']\n",
    "y = data['score']\n",
    "\n",
    "# Count occurrences of each class\n",
    "class_counts = Counter(y)\n",
    "\n",
    "# Define target number of samples per class\n",
    "target_samples = 10000\n",
    "\n",
    "# List to store undersampled data\n",
    "undersampled_data = []\n",
    "\n",
    "# Loop through each class and adjust samples\n",
    "for label in class_counts.keys():\n",
    "    if class_counts[label] >= target_samples:\n",
    "        # Take a random sample of size target_samples\n",
    "        undersampled_data.append(data[data['score'] == label].sample(target_samples, random_state=42))\n",
    "    else:\n",
    "        # If the class has fewer samples than target_samples, replicate the data\n",
    "        class_data = data[data['score'] == label]\n",
    "        # Calculate how many times to replicate and get the remainder\n",
    "        times_to_replicate = target_samples // class_counts[label]\n",
    "        remainder = target_samples % class_counts[label]\n",
    "        \n",
    "        # Create the replicated dataset\n",
    "        replicated_data = pd.concat([class_data] * times_to_replicate + [class_data.sample(remainder, random_state=42)])\n",
    "        undersampled_data.append(replicated_data)\n",
    "\n",
    "# Concatenate all undersampled data\n",
    "undersampled_data = pd.concat(undersampled_data)\n",
    "\n",
    "# Features and labels after undersampling\n",
    "X = undersampled_data['content']\n",
    "y = undersampled_data['score']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "847e6752-8777-4328-8d4e-a6a9bb4b8d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undersampled y_train distribution: Counter({5: 7524, 4: 7523, 3: 7510, 2: 7494, 1: 7449})\n",
      "Resampled y_train distribution: Counter({3: 7524, 4: 7524, 0: 7524, 2: 7524, 1: 7524})\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for text data\n",
    "max_words = 10000\n",
    "max_len = 150\n",
    "embedding_dim = 128\n",
    "\n",
    "# Tokenization and Padding\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Label Binarization\n",
    "lb = LabelBinarizer()\n",
    "y_train_onehot = lb.fit_transform(y_train)\n",
    "y_test_onehot = lb.transform(y_test)\n",
    "\n",
    "# Check distribution after undersampling\n",
    "print(f\"Undersampled y_train distribution: {Counter(y_train)}\")\n",
    "\n",
    "# Ensure y_train_onehot is compatible with SMOTE\n",
    "if len(set(y_train_onehot.flatten())) < 2:\n",
    "    print(\"Not enough classes for SMOTE.\")\n",
    "else:\n",
    "    # Resampling using SMOTE\n",
    "    smote = SMOTE(sampling_strategy='auto', random_state=42)  # Generates a balance\n",
    "    X_train_resampled, y_train_onehot_resampled = smote.fit_resample(X_train_padded, y_train_onehot)\n",
    "\n",
    "# Flattening for compatibility with models\n",
    "y_train_flat = y_train_onehot_resampled.argmax(axis=1)\n",
    "y_test_flat = y_test_onehot.argmax(axis=1)\n",
    "\n",
    "# Debugging: Check class distribution\n",
    "print(f\"Resampled y_train distribution: {Counter(y_train_flat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20deb890-4553-4c76-a0c3-b286acf71ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "588/588 [==============================] - 19s 27ms/step - loss: 1.5336 - accuracy: 0.3859 - val_loss: 1.2600 - val_accuracy: 0.3962\n",
      "Epoch 2/100\n",
      "588/588 [==============================] - 15s 26ms/step - loss: 1.2231 - accuracy: 0.4468 - val_loss: 1.1856 - val_accuracy: 0.4714\n",
      "Epoch 3/100\n",
      "588/588 [==============================] - 16s 27ms/step - loss: 1.1086 - accuracy: 0.5233 - val_loss: 1.1011 - val_accuracy: 0.5365\n",
      "Epoch 4/100\n",
      "588/588 [==============================] - 16s 27ms/step - loss: 1.0322 - accuracy: 0.5734 - val_loss: 1.0851 - val_accuracy: 0.5488\n",
      "Epoch 5/100\n",
      "588/588 [==============================] - 16s 26ms/step - loss: 0.9604 - accuracy: 0.6137 - val_loss: 1.0617 - val_accuracy: 0.5632\n",
      "Epoch 6/100\n",
      "588/588 [==============================] - 16s 27ms/step - loss: 0.8999 - accuracy: 0.6435 - val_loss: 1.0631 - val_accuracy: 0.5594\n",
      "Epoch 7/100\n",
      "588/588 [==============================] - 16s 27ms/step - loss: 0.8570 - accuracy: 0.6590 - val_loss: 1.0733 - val_accuracy: 0.5822\n",
      "Epoch 8/100\n",
      "588/588 [==============================] - 16s 27ms/step - loss: 0.8330 - accuracy: 0.6708 - val_loss: 1.0402 - val_accuracy: 0.5900\n",
      "Epoch 9/100\n",
      "588/588 [==============================] - 16s 26ms/step - loss: 0.8130 - accuracy: 0.6792 - val_loss: 1.0668 - val_accuracy: 0.5919\n",
      "Epoch 10/100\n",
      "588/588 [==============================] - 15s 26ms/step - loss: 0.7902 - accuracy: 0.6879 - val_loss: 1.0252 - val_accuracy: 0.6079\n",
      "Epoch 11/100\n",
      "588/588 [==============================] - 16s 27ms/step - loss: 0.7875 - accuracy: 0.6886 - val_loss: 1.0509 - val_accuracy: 0.6029\n",
      "Epoch 12/100\n",
      "588/588 [==============================] - 16s 28ms/step - loss: 0.7537 - accuracy: 0.6993 - val_loss: 1.0246 - val_accuracy: 0.6025\n",
      "Epoch 13/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.7248 - accuracy: 0.7091 - val_loss: 1.0127 - val_accuracy: 0.6137\n",
      "Epoch 14/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.7064 - accuracy: 0.7193 - val_loss: 1.0404 - val_accuracy: 0.6170\n",
      "Epoch 15/100\n",
      "588/588 [==============================] - 18s 30ms/step - loss: 0.6943 - accuracy: 0.7186 - val_loss: 1.0367 - val_accuracy: 0.6158\n",
      "Epoch 16/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.6869 - accuracy: 0.7230 - val_loss: 1.0383 - val_accuracy: 0.6277\n",
      "Epoch 17/100\n",
      "588/588 [==============================] - 17s 28ms/step - loss: 0.6707 - accuracy: 0.7280 - val_loss: 1.0502 - val_accuracy: 0.6254\n",
      "Epoch 18/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.6709 - accuracy: 0.7297 - val_loss: 1.0658 - val_accuracy: 0.6222\n",
      "Epoch 19/100\n",
      "588/588 [==============================] - 18s 30ms/step - loss: 0.6452 - accuracy: 0.7381 - val_loss: 1.0874 - val_accuracy: 0.6297\n",
      "Epoch 20/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.6374 - accuracy: 0.7393 - val_loss: 1.0598 - val_accuracy: 0.6295\n",
      "Epoch 21/100\n",
      "588/588 [==============================] - 17s 28ms/step - loss: 0.6326 - accuracy: 0.7453 - val_loss: 1.0647 - val_accuracy: 0.6262\n",
      "Epoch 22/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.6244 - accuracy: 0.7476 - val_loss: 1.0586 - val_accuracy: 0.6370\n",
      "Epoch 23/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.6191 - accuracy: 0.7498 - val_loss: 1.0898 - val_accuracy: 0.6420\n",
      "Epoch 24/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.6064 - accuracy: 0.7517 - val_loss: 1.0890 - val_accuracy: 0.6428\n",
      "Epoch 25/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.6050 - accuracy: 0.7544 - val_loss: 1.1359 - val_accuracy: 0.6422\n",
      "Epoch 26/100\n",
      "588/588 [==============================] - 18s 30ms/step - loss: 0.5909 - accuracy: 0.7585 - val_loss: 1.1161 - val_accuracy: 0.6394\n",
      "Epoch 27/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.5965 - accuracy: 0.7565 - val_loss: 1.0742 - val_accuracy: 0.6496\n",
      "Epoch 28/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.5901 - accuracy: 0.7606 - val_loss: 1.1068 - val_accuracy: 0.6509\n",
      "Epoch 29/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.5841 - accuracy: 0.7612 - val_loss: 1.1114 - val_accuracy: 0.6529\n",
      "Epoch 30/100\n",
      "588/588 [==============================] - 18s 30ms/step - loss: 0.5771 - accuracy: 0.7651 - val_loss: 1.1109 - val_accuracy: 0.6546\n",
      "Epoch 31/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.5739 - accuracy: 0.7652 - val_loss: 1.1177 - val_accuracy: 0.6553\n",
      "Epoch 32/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.5668 - accuracy: 0.7680 - val_loss: 1.1111 - val_accuracy: 0.6468\n",
      "Epoch 33/100\n",
      "588/588 [==============================] - 17s 30ms/step - loss: 0.5647 - accuracy: 0.7672 - val_loss: 1.1419 - val_accuracy: 0.6547\n",
      "Epoch 34/100\n",
      "588/588 [==============================] - 18s 30ms/step - loss: 0.5582 - accuracy: 0.7719 - val_loss: 1.1427 - val_accuracy: 0.6613\n",
      "Epoch 35/100\n",
      "588/588 [==============================] - 17s 28ms/step - loss: 0.5509 - accuracy: 0.7749 - val_loss: 1.1881 - val_accuracy: 0.6503\n",
      "Epoch 36/100\n",
      "588/588 [==============================] - 17s 28ms/step - loss: 0.5545 - accuracy: 0.7720 - val_loss: 1.1576 - val_accuracy: 0.6541\n",
      "Epoch 37/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.5543 - accuracy: 0.7742 - val_loss: 1.1407 - val_accuracy: 0.6614\n",
      "Epoch 38/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.5436 - accuracy: 0.7760 - val_loss: 1.1878 - val_accuracy: 0.6587\n",
      "Epoch 39/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.5450 - accuracy: 0.7764 - val_loss: 1.1780 - val_accuracy: 0.6620\n",
      "Epoch 40/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.5435 - accuracy: 0.7778 - val_loss: 1.2083 - val_accuracy: 0.6573\n",
      "Epoch 41/100\n",
      "588/588 [==============================] - 17s 30ms/step - loss: 0.5424 - accuracy: 0.7787 - val_loss: 1.1662 - val_accuracy: 0.6626\n",
      "Epoch 42/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.5318 - accuracy: 0.7813 - val_loss: 1.2005 - val_accuracy: 0.6555\n",
      "Epoch 43/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.5361 - accuracy: 0.7828 - val_loss: 1.1760 - val_accuracy: 0.6543\n",
      "Epoch 44/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.5250 - accuracy: 0.7839 - val_loss: 1.2260 - val_accuracy: 0.6588\n",
      "Epoch 45/100\n",
      "588/588 [==============================] - 17s 30ms/step - loss: 0.5303 - accuracy: 0.7829 - val_loss: 1.1950 - val_accuracy: 0.6502\n",
      "Epoch 46/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.5329 - accuracy: 0.7822 - val_loss: 1.2208 - val_accuracy: 0.6662\n",
      "Epoch 47/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.5210 - accuracy: 0.7877 - val_loss: 1.1920 - val_accuracy: 0.6672\n",
      "Epoch 48/100\n",
      "588/588 [==============================] - 17s 30ms/step - loss: 0.5261 - accuracy: 0.7837 - val_loss: 1.2301 - val_accuracy: 0.6677\n",
      "Epoch 49/100\n",
      "588/588 [==============================] - 18s 31ms/step - loss: 0.5211 - accuracy: 0.7879 - val_loss: 1.2280 - val_accuracy: 0.6662\n",
      "Epoch 50/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.5217 - accuracy: 0.7858 - val_loss: 1.2784 - val_accuracy: 0.6686\n",
      "Epoch 51/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.5126 - accuracy: 0.7910 - val_loss: 1.2250 - val_accuracy: 0.6718\n",
      "Epoch 52/100\n",
      "588/588 [==============================] - 18s 30ms/step - loss: 0.5105 - accuracy: 0.7896 - val_loss: 1.2065 - val_accuracy: 0.6740\n",
      "Epoch 53/100\n",
      "588/588 [==============================] - 17s 29ms/step - loss: 0.5035 - accuracy: 0.7945 - val_loss: 1.2676 - val_accuracy: 0.6679\n",
      "Epoch 54/100\n",
      "588/588 [==============================] - 18s 31ms/step - loss: 0.5179 - accuracy: 0.7889 - val_loss: 1.2322 - val_accuracy: 0.6691\n",
      "Epoch 55/100\n",
      "588/588 [==============================] - 18s 31ms/step - loss: 0.5114 - accuracy: 0.7907 - val_loss: 1.2266 - val_accuracy: 0.6689\n",
      "Epoch 56/100\n",
      "588/588 [==============================] - 16s 28ms/step - loss: 0.5020 - accuracy: 0.7926 - val_loss: 1.3339 - val_accuracy: 0.6703\n",
      "Epoch 57/100\n",
      "588/588 [==============================] - 18s 31ms/step - loss: 0.5014 - accuracy: 0.7944 - val_loss: 1.3777 - val_accuracy: 0.6580\n",
      "Epoch 58/100\n",
      "588/588 [==============================] - 18s 31ms/step - loss: 0.5080 - accuracy: 0.7946 - val_loss: 1.2774 - val_accuracy: 0.6663\n",
      "Epoch 59/100\n",
      "588/588 [==============================] - 22s 37ms/step - loss: 0.5056 - accuracy: 0.7959 - val_loss: 1.3292 - val_accuracy: 0.6741\n",
      "Epoch 60/100\n",
      "588/588 [==============================] - 22s 37ms/step - loss: 0.5032 - accuracy: 0.7947 - val_loss: 1.2964 - val_accuracy: 0.6714\n",
      "Epoch 61/100\n",
      "588/588 [==============================] - 17s 28ms/step - loss: 0.4987 - accuracy: 0.7980 - val_loss: 1.3355 - val_accuracy: 0.6735\n",
      "Epoch 62/100\n",
      "588/588 [==============================] - 17s 28ms/step - loss: 0.4958 - accuracy: 0.7953 - val_loss: 1.3558 - val_accuracy: 0.6718\n",
      "Epoch 63/100\n",
      "588/588 [==============================] - 18s 30ms/step - loss: 0.4872 - accuracy: 0.7994 - val_loss: 1.3822 - val_accuracy: 0.6747\n",
      "Epoch 64/100\n",
      "588/588 [==============================] - 122s 208ms/step - loss: 0.4947 - accuracy: 0.7990 - val_loss: 1.3292 - val_accuracy: 0.6742\n",
      "Epoch 65/100\n",
      "588/588 [==============================] - 56s 95ms/step - loss: 0.4936 - accuracy: 0.8001 - val_loss: 1.3448 - val_accuracy: 0.6738\n",
      "Epoch 66/100\n",
      "588/588 [==============================] - 21s 36ms/step - loss: 0.4890 - accuracy: 0.8017 - val_loss: 1.3096 - val_accuracy: 0.6745\n",
      "Epoch 67/100\n",
      "588/588 [==============================] - 21s 35ms/step - loss: 0.4878 - accuracy: 0.8001 - val_loss: 1.2539 - val_accuracy: 0.6751\n",
      "Epoch 68/100\n",
      "588/588 [==============================] - 21s 36ms/step - loss: 0.4840 - accuracy: 0.8027 - val_loss: 1.3405 - val_accuracy: 0.6745\n",
      "Epoch 69/100\n",
      "588/588 [==============================] - 22s 37ms/step - loss: 0.4939 - accuracy: 0.7990 - val_loss: 1.2638 - val_accuracy: 0.6774\n",
      "Epoch 70/100\n",
      "588/588 [==============================] - 22s 38ms/step - loss: 0.4765 - accuracy: 0.8041 - val_loss: 1.3539 - val_accuracy: 0.6822\n",
      "Epoch 71/100\n",
      "588/588 [==============================] - 22s 37ms/step - loss: 0.4809 - accuracy: 0.8020 - val_loss: 1.5262 - val_accuracy: 0.6606\n",
      "Epoch 72/100\n",
      "588/588 [==============================] - 22s 38ms/step - loss: 0.4874 - accuracy: 0.8030 - val_loss: 1.3585 - val_accuracy: 0.6778\n",
      "Epoch 73/100\n",
      "588/588 [==============================] - 23s 39ms/step - loss: 0.4811 - accuracy: 0.8053 - val_loss: 1.3720 - val_accuracy: 0.6797\n",
      "Epoch 74/100\n",
      "588/588 [==============================] - 22s 38ms/step - loss: 0.4825 - accuracy: 0.8052 - val_loss: 1.3737 - val_accuracy: 0.6788\n",
      "Epoch 75/100\n",
      "588/588 [==============================] - 22s 38ms/step - loss: 0.4798 - accuracy: 0.8071 - val_loss: 1.3354 - val_accuracy: 0.6786\n",
      "Epoch 76/100\n",
      "588/588 [==============================] - 22s 38ms/step - loss: 0.4715 - accuracy: 0.8067 - val_loss: 1.3861 - val_accuracy: 0.6748\n",
      "Epoch 77/100\n",
      "588/588 [==============================] - 22s 37ms/step - loss: 0.4734 - accuracy: 0.8084 - val_loss: 1.3072 - val_accuracy: 0.6816\n",
      "Epoch 78/100\n",
      "588/588 [==============================] - 23s 39ms/step - loss: 0.4812 - accuracy: 0.8055 - val_loss: 1.3431 - val_accuracy: 0.6853\n",
      "Epoch 79/100\n",
      "588/588 [==============================] - 24s 41ms/step - loss: 0.4725 - accuracy: 0.8077 - val_loss: 1.3556 - val_accuracy: 0.6824\n",
      "Epoch 80/100\n",
      "588/588 [==============================] - 23s 39ms/step - loss: 0.4692 - accuracy: 0.8079 - val_loss: 1.3302 - val_accuracy: 0.6850\n",
      "Epoch 81/100\n",
      "588/588 [==============================] - 23s 39ms/step - loss: 0.4698 - accuracy: 0.8105 - val_loss: 1.3767 - val_accuracy: 0.6813\n",
      "Epoch 82/100\n",
      "588/588 [==============================] - 24s 40ms/step - loss: 0.4721 - accuracy: 0.8095 - val_loss: 1.3907 - val_accuracy: 0.6843\n",
      "Epoch 83/100\n",
      "588/588 [==============================] - 23s 39ms/step - loss: 0.4643 - accuracy: 0.8127 - val_loss: 1.4496 - val_accuracy: 0.6798\n",
      "Epoch 84/100\n",
      "588/588 [==============================] - 23s 39ms/step - loss: 0.4829 - accuracy: 0.8072 - val_loss: 1.3410 - val_accuracy: 0.6807\n",
      "Epoch 85/100\n",
      "588/588 [==============================] - 23s 39ms/step - loss: 0.4643 - accuracy: 0.8146 - val_loss: 1.3861 - val_accuracy: 0.6850\n",
      "Epoch 86/100\n",
      "588/588 [==============================] - 23s 40ms/step - loss: 0.4816 - accuracy: 0.8089 - val_loss: 1.3012 - val_accuracy: 0.6817\n",
      "Epoch 87/100\n",
      "588/588 [==============================] - 35s 59ms/step - loss: 0.4647 - accuracy: 0.8132 - val_loss: 1.4060 - val_accuracy: 0.6836\n",
      "Epoch 88/100\n",
      "588/588 [==============================] - 36s 61ms/step - loss: 0.4538 - accuracy: 0.8155 - val_loss: 1.3787 - val_accuracy: 0.6894\n",
      "Epoch 89/100\n",
      "588/588 [==============================] - 41s 69ms/step - loss: 0.4687 - accuracy: 0.8119 - val_loss: 1.3765 - val_accuracy: 0.6872\n",
      "Epoch 90/100\n",
      "588/588 [==============================] - 40s 68ms/step - loss: 0.4833 - accuracy: 0.8077 - val_loss: 1.3360 - val_accuracy: 0.6880\n",
      "Epoch 91/100\n",
      "588/588 [==============================] - 40s 68ms/step - loss: 0.4429 - accuracy: 0.8190 - val_loss: 1.5165 - val_accuracy: 0.6877\n",
      "Epoch 92/100\n",
      "588/588 [==============================] - 38s 65ms/step - loss: 0.4630 - accuracy: 0.8132 - val_loss: 1.4013 - val_accuracy: 0.6798\n",
      "Epoch 93/100\n",
      "588/588 [==============================] - 36s 61ms/step - loss: 0.4505 - accuracy: 0.8168 - val_loss: 1.4655 - val_accuracy: 0.6878\n",
      "Epoch 94/100\n",
      "588/588 [==============================] - 32s 54ms/step - loss: 0.4549 - accuracy: 0.8159 - val_loss: 1.4687 - val_accuracy: 0.6824\n",
      "Epoch 95/100\n",
      "588/588 [==============================] - 22s 38ms/step - loss: 0.4706 - accuracy: 0.8116 - val_loss: 1.3978 - val_accuracy: 0.6840\n",
      "Epoch 96/100\n",
      "588/588 [==============================] - 22s 37ms/step - loss: 0.4525 - accuracy: 0.8186 - val_loss: 1.4745 - val_accuracy: 0.6890\n",
      "Epoch 97/100\n",
      "588/588 [==============================] - 24s 41ms/step - loss: 0.4673 - accuracy: 0.8147 - val_loss: 1.4134 - val_accuracy: 0.6798\n",
      "Epoch 98/100\n",
      "588/588 [==============================] - 22s 38ms/step - loss: 0.4671 - accuracy: 0.8149 - val_loss: 1.4662 - val_accuracy: 0.6912\n",
      "Epoch 99/100\n",
      "588/588 [==============================] - 23s 39ms/step - loss: 0.4630 - accuracy: 0.8180 - val_loss: 1.4417 - val_accuracy: 0.6904\n",
      "Epoch 100/100\n",
      "588/588 [==============================] - 22s 38ms/step - loss: 0.4441 - accuracy: 0.8211 - val_loss: 1.4652 - val_accuracy: 0.6889\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.4652 - accuracy: 0.6889\n",
      "391/391 [==============================] - 7s 14ms/step\n",
      "LSTM Test Accuracy: 0.6888800263404846\n",
      "LSTM Precision: 0.692898426530578\n",
      "LSTM Recall: 0.68888\n",
      "LSTM F1-Score: 0.6860677938826409\n"
     ]
    }
   ],
   "source": [
    "# --- Define and Train the LSTM Model ---\n",
    "lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(max_words, embedding_dim, input_length=max_len),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.01))),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(y_train_onehot.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "lstm_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the LSTM model\n",
    "lstm_model.fit(X_train_resampled, y_train_onehot_resampled, epochs=100, batch_size=64,\n",
    "               validation_data=(X_test_padded, y_test_onehot), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the LSTM model\n",
    "lstm_test_loss, lstm_test_accuracy = lstm_model.evaluate(X_test_padded, y_test_onehot)\n",
    "\n",
    "# Predict with the LSTM model\n",
    "y_pred_lstm = lstm_model.predict(X_test_padded).argmax(axis=1)\n",
    "\n",
    "# LSTM metrics\n",
    "lstm_precision = precision_score(y_test_flat, y_pred_lstm, average='weighted')\n",
    "lstm_recall = recall_score(y_test_flat, y_pred_lstm, average='weighted')\n",
    "lstm_f1 = f1_score(y_test_flat, y_pred_lstm, average='weighted')\n",
    "\n",
    "print(f\"LSTM Test Accuracy: {lstm_test_accuracy}\")\n",
    "print(f\"LSTM Precision: {lstm_precision}\")\n",
    "print(f\"LSTM Recall: {lstm_recall}\")\n",
    "print(f\"LSTM F1-Score: {lstm_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b01febf2-8d23-4e8a-bbb7-3d103b9d48bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kurotsuki\\PycharmProjects\\mobile_jkn_baru\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Kurotsuki\\PycharmProjects\\mobile_jkn_baru\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# --- Random Forest Model ---\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predict with Random Forest\n",
    "y_pred_rf = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# Random Forest metrics\n",
    "rf_accuracy = accuracy_score(y_test_flat, y_pred_rf)\n",
    "rf_precision = precision_score(y_test_flat, y_pred_rf, average='weighted')\n",
    "rf_recall = recall_score(y_test_flat, y_pred_rf, average='weighted')\n",
    "rf_f1 = f1_score(y_test_flat, y_pred_rf, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "498680af-e6a0-46e3-921c-096ef1539f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_resampled: (37620, 1000)\n",
      "Shape of y_resampled: (37620,)\n",
      "Accuracy: 0.6904\n",
      "Precision: 0.6951216888227025\n",
      "Recall: 0.6904\n",
      "F1 Score: 0.682103786261003\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray()\n",
    "\n",
    "# Reshape y_train agar sesuai dengan dimensi X_train_tfidf\n",
    "y_train_flat = y_train.values.flatten()  # pastikan y_train_flat adalah array 1D\n",
    "\n",
    "# Terapkan SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Lakukan resampling pada X_train_tfidf dan y_train_flat\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_tfidf, y_train_flat)\n",
    "\n",
    "# Lihat hasil setelah SMOTE\n",
    "print(f\"Shape of X_resampled: {X_resampled.shape}\")\n",
    "print(f\"Shape of y_resampled: {y_resampled.shape}\")\n",
    "\n",
    "# Proses pelatihan model\n",
    "# Misalnya menggunakan SVM atau model lain yang Anda pilih\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Prediksi dan evaluasi\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluasi model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d48fd1af-a409-4cef-aa59-b0f43c7b094f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.56928\n",
      "XGBoost Precision: 0.5671231562907131\n",
      "XGBoost Recall: 0.56928\n",
      "XGBoost F1 Score: 0.5613871805874229\n"
     ]
    }
   ],
   "source": [
    "# Normalisasi label agar mulai dari 0\n",
    "y_resampled_normalized = y_resampled - 1  # Jika kelas mulai dari 1 hingga 5\n",
    "\n",
    "# --------------------- XGBoost Model ---------------------\n",
    "xgb_model = XGBClassifier(eval_metric='mlogloss')  # Fine-tune hyperparameters di sini\n",
    "xgb_model.fit(X_resampled, y_resampled_normalized)  # Menggunakan label yang sudah dinormalisasi\n",
    "\n",
    "# Prediksi dengan XGBoost\n",
    "y_pred_xgb = xgb_model.predict(X_test_tfidf)\n",
    "\n",
    "# XGBoost Metrics\n",
    "xgb_accuracy = accuracy_score(y_test - 1, y_pred_xgb)  # Normalisasi y_test jika perlu\n",
    "xgb_precision = precision_score(y_test - 1, y_pred_xgb, average='weighted')\n",
    "xgb_recall = recall_score(y_test - 1, y_pred_xgb, average='weighted')\n",
    "xgb_f1 = f1_score(y_test - 1, y_pred_xgb, average='weighted')\n",
    "\n",
    "print(\"XGBoost Accuracy:\", xgb_accuracy)\n",
    "print(\"XGBoost Precision:\", xgb_precision)\n",
    "print(\"XGBoost Recall:\", xgb_recall)\n",
    "print(\"XGBoost F1 Score:\", xgb_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c2cdec4-74b0-4493-bc2c-8d6f4ed3cd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes Accuracy with SMOTE: 0.16816\n",
      "Naive Bayes Precision with SMOTE: 0.17953746186405525\n",
      "Naive Bayes Recall with SMOTE: 0.16816\n",
      "Naive Bayes F1 Score with SMOTE: 0.16810490673694825\n"
     ]
    }
   ],
   "source": [
    "# --------------------- Naive Bayes Model with SMOTE ---------------------\n",
    "nb_model = MultinomialNB(alpha=0.001)  # Adjust alpha for smoothing if necessary\n",
    "nb_model.fit(X_resampled, y_resampled)  # Menggunakan data hasil SMOTE\n",
    "\n",
    "# Predict with Naive Bayes\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "# Naive Bayes Metrics\n",
    "nb_accuracy = accuracy_score(y_test_flat, y_pred_nb)\n",
    "nb_precision = precision_score(y_test_flat, y_pred_nb, average='weighted', zero_division=0)\n",
    "nb_recall = recall_score(y_test_flat, y_pred_nb, average='weighted', zero_division=0)\n",
    "nb_f1 = f1_score(y_test_flat, y_pred_nb, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"\\nNaive Bayes Accuracy with SMOTE:\", nb_accuracy)\n",
    "print(\"Naive Bayes Precision with SMOTE:\", nb_precision)\n",
    "print(\"Naive Bayes Recall with SMOTE:\", nb_recall)\n",
    "print(\"Naive Bayes F1 Score with SMOTE:\", nb_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63f5a2-b2c8-4bed-b482-f8cb36cd4164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Results ---\n",
    "results = {\n",
    "    \"Model\": [\"LSTM\", \"Random Forest\", \"XGBoost\", \"Naive Bayes\"],\n",
    "    \"Accuracy\": [lstm_test_accuracy, rf_accuracy, xgb_accuracy, accuracy],\n",
    "    \"Precision\": [lstm_precision, rf_precision, xgb_precision, precision],\n",
    "    \"Recall\": [lstm_recall, rf_recall, xgb_recall, recall],\n",
    "    \"F1-Score\": [lstm_f1, rf_f1, xgb_f1, f1]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the DataFrame as a table\n",
    "print(\"\\nComparison of Model Results With SMOTE:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca712f2-df85-4edc-a879-f38e0065110b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
