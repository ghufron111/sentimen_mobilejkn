{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "012807dc-bda0-4e54-b8ed-36efc78ce676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2631741b-0be1-48f2-a8ce-079081a55ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load dataset\n",
    "# df = pd.read_csv('dataset/mobile_jkn.csv')\n",
    "\n",
    "# # Data preprocessing\n",
    "# X = df['content']  # Assuming 'review' column has the text\n",
    "# y = df['score']  # Assuming 'rating' column has the labels\n",
    "\n",
    "# # Encode labels\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Tentukan jumlah kata maksimal dan panjang sekuens maksimal\n",
    "# max_words = 10000\n",
    "# max_len = 100\n",
    "\n",
    "# # Buat tokenizer\n",
    "# tokenizer = Tokenizer(num_words=max_words)\n",
    "# tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# # Ubah teks menjadi sekuens\n",
    "# X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "# X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# # Padding sekuens agar semua input memiliki panjang yang sama\n",
    "# X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "# X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "# # Periksa hasil padding\n",
    "# print(X_train_pad.shape, X_test_pad.shape)\n",
    "\n",
    "# # Function to evaluate models\n",
    "# def evaluate_model(y_true, y_pred):\n",
    "#     accuracy = accuracy_score(y_true, y_pred)\n",
    "#     precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "#     recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "#     f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "#     return accuracy, precision, recall, f1\n",
    "\n",
    "# # Results dictionary\n",
    "# results = {\n",
    "#     'Model': [],\n",
    "#     'Accuracy': [],\n",
    "#     'Precision': [],\n",
    "#     'Recall': [],\n",
    "#     'F1 Score': []\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5cfbdb8-5ae6-4727-ad97-9981cbbf1345",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.22 GiB for an array with shape (70000, 10000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# TF-IDF Vectorization\u001b[39;00m\n\u001b[0;32m     16\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)  \u001b[38;5;66;03m# Limit to 10,000 words\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m X_train_tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Convert to TF-IDF features (dense array)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m X_test_tfidf \u001b[38;5;241m=\u001b[39m tfidf\u001b[38;5;241m.\u001b[39mtransform(X_test)\u001b[38;5;241m.\u001b[39mtoarray()  \u001b[38;5;66;03m# Apply TF-IDF on test data\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Padding sequences to ensure the input to LSTM has a consistent length (required for RNN models)\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\mobile_jkn_baru\\.venv\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1051\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1050\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1051\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\PycharmProjects\\mobile_jkn_baru\\.venv\\lib\\site-packages\\scipy\\sparse\\_base.py:1298\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 5.22 GiB for an array with shape (70000, 10000) and data type float64"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('dataset/mobile_jkn.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "X = df['content']  # Assuming 'content' column has the text data\n",
    "y = df['score']  # Assuming 'score' column has the target labels\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=10000)  # Limit to 10,000 words\n",
    "X_train_tfidf = tfidf.fit_transform(X_train).toarray()  # Convert to TF-IDF features (dense array)\n",
    "X_test_tfidf = tfidf.transform(X_test).toarray()  # Apply TF-IDF on test data\n",
    "\n",
    "# Padding sequences to ensure the input to LSTM has a consistent length (required for RNN models)\n",
    "max_len = 100  # Maximum length for padding (this can be adjusted based on your data)\n",
    "\n",
    "# Since TF-IDF already outputs dense features, we don't need padding directly, but can truncate/pad the sequences\n",
    "X_train_pad = pad_sequences(X_train_tfidf, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_tfidf, maxlen=max_len)\n",
    "\n",
    "# If using SMOTE (Optional)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_pad, y_train)\n",
    "\n",
    "# Check the shapes of padded data\n",
    "print(X_train_pad.shape, X_test_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f64f3c-e4e8-43ee-99ef-159153fa6a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model LSTM tanpa SMOTE\n",
    "model_without_smote = Sequential()\n",
    "model_without_smote.add(Embedding(max_words, 128, input_length=max_len))  # Layer embedding\n",
    "model_without_smote.add(LSTM(100))  # Layer LSTM\n",
    "model_without_smote.add(Dense(2, activation='softmax'))  # Output layer untuk klasifikasi\n",
    "\n",
    "# Kompilasi model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model_without_smote.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model_without_smote.fit(X_train_pad, y_train, epochs=5, batch_size=64, verbose=0)\n",
    "\n",
    "# Prediksi dan evaluasi model\n",
    "y_pred_lstm_without_smote = np.argmax(model_without_smote.predict(X_test_pad), axis=1)\n",
    "accuracy, precision, recall, f1 = evaluate_model(y_test, y_pred_lstm_without_smote)\n",
    "\n",
    "results['Model'].append('LSTM (Without SMOTE)')\n",
    "results['Accuracy'].append(accuracy)\n",
    "results['Precision'].append(precision)\n",
    "results['Recall'].append(recall)\n",
    "results['F1 Score'].append(f1)\n",
    "\n",
    "# Terapkan SMOTE pada data yang sudah di-padding\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_pad, y_train)\n",
    "\n",
    "# Model LSTM dengan SMOTE\n",
    "model_with_smote = Sequential()\n",
    "model_with_smote.add(Embedding(max_words, 128, input_length=max_len))  # Layer embedding\n",
    "model_with_smote.add(LSTM(100))  # Layer LSTM\n",
    "model_with_smote.add(Dense(2, activation='softmax'))  # Output layer untuk klasifikasi\n",
    "\n",
    "# Kompilasi model\n",
    "model_with_smote.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model_with_smote.fit(X_train_res, y_train_res, epochs=5, batch_size=64, verbose=0)\n",
    "\n",
    "# Prediksi dan evaluasi model\n",
    "y_pred_lstm_with_smote = np.argmax(model_with_smote.predict(X_test_pad), axis=1)\n",
    "accuracy, precision, recall, f1 = evaluate_model(y_test, y_pred_lstm_with_smote)\n",
    "\n",
    "results['Model'].append('LSTM (With SMOTE)')\n",
    "results['Accuracy'].append(accuracy)\n",
    "results['Precision'].append(precision)\n",
    "results['Recall'].append(recall)\n",
    "results['F1 Score'].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472b0ed6-4235-43e0-ad09-6616e81b2281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Definisikan jumlah maksimal fitur yang akan digunakan oleh TF-IDF\n",
    "# max_words = 10000\n",
    "\n",
    "# # Buat vectorizer TF-IDF\n",
    "# tfidf = TfidfVectorizer(max_features=max_words)\n",
    "\n",
    "# # Transformasikan data teks menjadi vektor TF-IDF tanpa mengkonversi ke array\n",
    "# X_tfidf = tfidf.fit_transform(df['content'])  # Kembali ke sparse matrix\n",
    "# y = df['score']\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0e5a26-f1f8-4585-8f6d-1b8108e4e7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes tanpa SMOTE\n",
    "nb_model_without_smote = MultinomialNB()\n",
    "nb_model_without_smote.fit(X_train, y_train)  # Menggunakan data hasil TF-IDF\n",
    "\n",
    "# Prediksi\n",
    "y_pred_nb_without_smote = nb_model_without_smote.predict(X_test)\n",
    "\n",
    "# Evaluasi model\n",
    "accuracy, precision, recall, f1 = evaluate_model(y_test, y_pred_nb_without_smote)\n",
    "\n",
    "# Simpan hasil ke dalam dictionary\n",
    "results['Model'].append('Naive Bayes (Without SMOTE)')\n",
    "results['Accuracy'].append(accuracy)\n",
    "results['Precision'].append(precision)\n",
    "results['Recall'].append(recall)\n",
    "results['F1 Score'].append(f1)\n",
    "\n",
    "# Terapkan SMOTE pada data hasil TF-IDF\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Naive Bayes dengan SMOTE\n",
    "nb_model_with_smote = MultinomialNB()\n",
    "nb_model_with_smote.fit(X_train_res, y_train_res)  # Menggunakan data yang diresample\n",
    "\n",
    "# Prediksi\n",
    "y_pred_nb_with_smote = nb_model_with_smote.predict(X_test)\n",
    "\n",
    "# Evaluasi model\n",
    "accuracy, precision, recall, f1 = evaluate_model(y_test, y_pred_nb_with_smote)\n",
    "\n",
    "# Simpan hasil ke dalam dictionary\n",
    "results['Model'].append('Naive Bayes (With SMOTE)')\n",
    "results['Accuracy'].append(accuracy)\n",
    "results['Precision'].append(precision)\n",
    "results['Recall'].append(recall)\n",
    "results['F1 Score'].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b9553d-9221-4b23-9538-b399ed3f7c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Definisikan jumlah maksimal fitur yang akan digunakan oleh TF-IDF\n",
    "# max_words = 10000\n",
    "\n",
    "# # Buat vectorizer TF-IDF\n",
    "# tfidf = TfidfVectorizer(max_features=max_words)\n",
    "\n",
    "# # Transformasikan data teks menjadi vektor TF-IDF\n",
    "# X_tfidf = tfidf.fit_transform(df['content']).toarray()  # Preprocess 'content'\n",
    "# y = df['score']\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1b273-90e8-4f7b-aef3-01d6466486a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest tanpa SMOTE\n",
    "rf_model_without_smote = RandomForestClassifier(random_state=42)\n",
    "rf_model_without_smote.fit(X_train, y_train)  # Menggunakan data hasil TF-IDF\n",
    "\n",
    "# Prediksi\n",
    "y_pred_rf_without_smote = rf_model_without_smote.predict(X_test)\n",
    "\n",
    "# Evaluasi model\n",
    "accuracy, precision, recall, f1 = evaluate_model(y_test, y_pred_rf_without_smote)\n",
    "\n",
    "# Simpan hasil ke dalam dictionary\n",
    "results['Model'].append('Random Forest (Without SMOTE)')\n",
    "results['Accuracy'].append(accuracy)\n",
    "results['Precision'].append(precision)\n",
    "results['Recall'].append(recall)\n",
    "results['F1 Score'].append(f1)\n",
    "\n",
    "# Terapkan SMOTE pada data hasil TF-IDF\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Random Forest dengan SMOTE\n",
    "rf_model_with_smote = RandomForestClassifier(random_state=42)\n",
    "rf_model_with_smote.fit(X_train_res, y_train_res)  # Menggunakan data yang diresample\n",
    "\n",
    "# Prediksi\n",
    "y_pred_rf_with_smote = rf_model_with_smote.predict(X_test)\n",
    "\n",
    "# Evaluasi model\n",
    "accuracy, precision, recall, f1 = evaluate_model(y_test, y_pred_rf_with_smote)\n",
    "\n",
    "# Simpan hasil ke dalam dictionary\n",
    "results['Model'].append('Random Forest (With SMOTE)')\n",
    "results['Accuracy'].append(accuracy)\n",
    "results['Precision'].append(precision)\n",
    "results['Recall'].append(recall)\n",
    "results['F1 Score'].append(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62606e5-b0b9-4bdf-b018-72384f7d01ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust labels if necessary\n",
    "y_train = y_train - 1\n",
    "y_test = y_test - 1\n",
    "\n",
    "# XGBoost tanpa SMOTE\n",
    "xgb_model_without_smote = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "xgb_model_without_smote.fit(X_train, y_train)  # Menggunakan data hasil TF-IDF\n",
    "\n",
    "# Prediksi\n",
    "y_pred_xgb_without_smote = xgb_model_without_smote.predict(X_test)\n",
    "\n",
    "# Evaluasi model (assumes you have evaluate_model function defined)\n",
    "accuracy, precision, recall, f1 = evaluate_model(y_test, y_pred_xgb_without_smote)\n",
    "\n",
    "# Simpan hasil ke dalam dictionary\n",
    "results['Model'].append('XGBoost (Without SMOTE)')\n",
    "results['Accuracy'].append(accuracy)\n",
    "results['Precision'].append(precision)\n",
    "results['Recall'].append(recall)\n",
    "results['F1 Score'].append(f1)\n",
    "\n",
    "# Terapkan SMOTE pada data hasil TF-IDF\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# XGBoost dengan SMOTE\n",
    "xgb_model_with_smote = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "xgb_model_with_smote.fit(X_train_res, y_train_res)  # Menggunakan data yang di-resample\n",
    "\n",
    "# Prediksi\n",
    "y_pred_xgb_with_smote = xgb_model_with_smote.predict(X_test)\n",
    "\n",
    "# Evaluasi model\n",
    "accuracy, precision, recall, f1 = evaluate_model(y_test, y_pred_xgb_with_smote)\n",
    "\n",
    "# Simpan hasil ke dalam dictionary\n",
    "results['Model'].append('XGBoost (With SMOTE)')\n",
    "results['Accuracy'].append(accuracy)\n",
    "results['Precision'].append(precision)\n",
    "results['Recall'].append(recall)\n",
    "results['F1 Score'].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d12a3-47ea-4a49-a1cb-2c0ab055e68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0f6149-3af9-4744-8d54-68767e39bc0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
