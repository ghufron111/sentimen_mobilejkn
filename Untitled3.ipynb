{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c82add5-86d9-44e1-bcee-4fd8bc7570d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Results:\n",
      "Accuracy: 0.6057108140947752\n",
      "Precision: 0.3339582280784015\n",
      "Recall: 0.28098851091420574\n",
      "F1 Score: 0.2586715881349586\n",
      "\n",
      "XGBoost Results:\n",
      "Accuracy: 0.7008505467800729\n",
      "Precision: 0.38234736127905145\n",
      "Recall: 0.39185431480754584\n",
      "F1 Score: 0.38309478135705277\n",
      "\n",
      "Random Forest Results:\n",
      "Accuracy: 0.7055893074119076\n",
      "Precision: 0.3565844040094506\n",
      "Recall: 0.3696105286848156\n",
      "F1 Score: 0.3567225814042015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('dataset/mobile_jkn.csv')\n",
    "\n",
    "# Preprocess text data\n",
    "data['content'] = data['content'].apply(preprocess_text)\n",
    "\n",
    "# Remove empty content\n",
    "data = data[data['content'].str.strip() != '']  # Remove empty reviews\n",
    "\n",
    "# Features and labels\n",
    "X = data['content']\n",
    "y = data['score']\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Hyperparameters for text data\n",
    "max_words = 10000\n",
    "max_len = 150\n",
    "\n",
    "# Tokenization and Padding\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Label Binarization\n",
    "lb = LabelBinarizer()\n",
    "y_train_onehot = lb.fit_transform(y_train)\n",
    "y_test_onehot = lb.transform(y_test)\n",
    "\n",
    "# Flatten y_train and y_test for model compatibility\n",
    "y_train_flat = y_train_onehot.argmax(axis=1)\n",
    "y_test_flat = y_test_onehot.argmax(axis=1)\n",
    "\n",
    "# Apply SMOTE to handle imbalanced data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_padded, y_train_flat)\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_test_padded, y_test_flat):\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    y_pred = model.predict(X_test_padded)\n",
    "    accuracy = accuracy_score(y_test_flat, y_pred)\n",
    "    precision = precision_score(y_test_flat, y_pred, average='macro')\n",
    "    recall = recall_score(y_test_flat, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test_flat, y_pred, average='macro')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Initialize models\n",
    "nb_model = MultinomialNB()\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Evaluate Naive Bayes\n",
    "nb_accuracy, nb_precision, nb_recall, nb_f1 = evaluate_model(nb_model, X_test_padded, y_test_flat)\n",
    "print(f\"Naive Bayes Results:\\nAccuracy: {nb_accuracy}\\nPrecision: {nb_precision}\\nRecall: {nb_recall}\\nF1 Score: {nb_f1}\\n\")\n",
    "\n",
    "# Evaluate XGBoost\n",
    "xgb_accuracy, xgb_precision, xgb_recall, xgb_f1 = evaluate_model(xgb_model, X_test_padded, y_test_flat)\n",
    "print(f\"XGBoost Results:\\nAccuracy: {xgb_accuracy}\\nPrecision: {xgb_precision}\\nRecall: {xgb_recall}\\nF1 Score: {xgb_f1}\\n\")\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_accuracy, rf_precision, rf_recall, rf_f1 = evaluate_model(rf_model, X_test_padded, y_test_flat)\n",
    "print(f\"Random Forest Results:\\nAccuracy: {rf_accuracy}\\nPrecision: {rf_precision}\\nRecall: {rf_recall}\\nF1 Score: {rf_f1}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b0ac87c-df18-4010-9398-5ccc4e74f7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique classes in y_train_resampled: 5\n"
     ]
    }
   ],
   "source": [
    "# Check the number of unique classes in the training data\n",
    "print(f\"Number of unique classes in y_train_resampled: {len(np.unique(y_train_resampled))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f9ac247-58ed-4caf-ad3a-e4912d52296a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3863/3863 - 134s - loss: 1.1877 - accuracy: 0.4589 - val_loss: 0.7363 - val_accuracy: 0.7889 - 134s/epoch - 35ms/step\n",
      "Epoch 2/30\n",
      "3863/3863 - 125s - loss: 1.1032 - accuracy: 0.4973 - val_loss: 0.7031 - val_accuracy: 0.7973 - 125s/epoch - 32ms/step\n",
      "Epoch 3/30\n",
      "3863/3863 - 122s - loss: 1.0595 - accuracy: 0.5199 - val_loss: 0.7004 - val_accuracy: 0.8050 - 122s/epoch - 32ms/step\n",
      "Epoch 4/30\n",
      "3863/3863 - 128s - loss: 1.0191 - accuracy: 0.5429 - val_loss: 0.7482 - val_accuracy: 0.7864 - 128s/epoch - 33ms/step\n",
      "Epoch 5/30\n",
      "3863/3863 - 126s - loss: 0.9769 - accuracy: 0.5653 - val_loss: 0.7368 - val_accuracy: 0.7847 - 126s/epoch - 33ms/step\n",
      "Epoch 6/30\n",
      "3863/3863 - 124s - loss: 0.9321 - accuracy: 0.5884 - val_loss: 0.7962 - val_accuracy: 0.7653 - 124s/epoch - 32ms/step\n",
      "LSTM Test Loss: 0.7004\n",
      "LSTM Test Accuracy: 0.8050\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('dataset/mobile_jkn.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "X = data['content']  # Review text\n",
    "y = data['score']    # Sentiment labels\n",
    "\n",
    "# Update the encoding to keep original class names\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Text preprocessing\n",
    "max_length = 100  # Set maximum length of sequences\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)  # Fit on the original training data\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length)\n",
    "\n",
    "# SMOTE for oversampling\n",
    "smote = SMOTE()\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_padded, y_train)\n",
    "\n",
    "# LSTM model\n",
    "def create_lstm_model(input_length, vocab_size, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=input_length))\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True)))  # Use Bidirectional LSTM\n",
    "    model.add(Dropout(0.3))  # Increase dropout rate\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # Use number of unique classes\n",
    "    return model\n",
    "\n",
    "input_length = X_train_padded.shape[1]  # Length of padded sequences\n",
    "vocab_size = 10000  # Update this to your actual vocab size\n",
    "num_classes = len(np.unique(y_encoded))  # Get the number of unique classes\n",
    "lstm_model = create_lstm_model(input_length, vocab_size, num_classes)\n",
    "\n",
    "# Compile model\n",
    "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the LSTM model\n",
    "history = lstm_model.fit(X_train_resampled, y_train_resampled,\n",
    "                         validation_data=(X_test_padded, y_test),\n",
    "                         epochs=30, \n",
    "                         batch_size=64,\n",
    "                         callbacks=[early_stopping],\n",
    "                         verbose=2)\n",
    "\n",
    "# Evaluate LSTM model\n",
    "lstm_loss, lstm_accuracy = lstm_model.evaluate(X_test_padded, y_test, verbose=0)\n",
    "print(f\"LSTM Test Loss: {lstm_loss:.4f}\")\n",
    "print(f\"LSTM Test Accuracy: {lstm_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25791ac2-211c-4194-b01c-a8b5ee8bb29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 9s 13ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.89      0.82      5139\n",
      "           2       0.10      0.05      0.06       767\n",
      "           3       0.11      0.06      0.08       599\n",
      "           4       0.16      0.13      0.14      1119\n",
      "           5       0.92      0.91      0.92     12376\n",
      "\n",
      "    accuracy                           0.81     20000\n",
      "   macro avg       0.41      0.41      0.40     20000\n",
      "weighted avg       0.78      0.81      0.79     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions for classification report\n",
    "y_pred_lstm = lstm_model.predict(X_test_padded)\n",
    "y_pred_classes = np.argmax(y_pred_lstm, axis=1)  # Convert predictions to class labels\n",
    "\n",
    "# Get original class names from label encoder as strings\n",
    "class_names = label_encoder.classes_.astype(str)\n",
    "\n",
    "# Calculate and print classification report\n",
    "report = classification_report(y_test, y_pred_classes, target_names=class_names)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d3026-a3e6-4c1f-9c39-cef9fca073da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
