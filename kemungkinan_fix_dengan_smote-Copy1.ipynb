{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1821b8c-c599-402f-94c4-a98136e5d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ac447bd-bf7e-40f1-a165-7ea21b39965f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kurotsuki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kurotsuki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kurotsuki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function for preprocessing text\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('dataset/mobile_jkn.csv')  # Load your CSV file\n",
    "\n",
    "# Preprocess text data\n",
    "data['content'] = data['content'].apply(preprocess_text)\n",
    "\n",
    "# Remove empty content\n",
    "data = data[data['content'].str.strip() != '']  # Remove empty reviews\n",
    "\n",
    "# Features and labels\n",
    "X = data['content']\n",
    "y = data['score']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e6752-8777-4328-8d4e-a6a9bb4b8d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Vektorisasi data teks\n",
    "vectorizer = TfidfVectorizer(max_features=10)  # Mengurangi jumlah fitur\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Mengonversi ke dense matrix dengan hati-hati\n",
    "X_train_dense = X_train_vectorized.toarray()  # Memastikan data kecil\n",
    "\n",
    "# Resampling menggunakan SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_dense, y_train)\n",
    "\n",
    "# Hyperparameters\n",
    "max_words = 5000  # Untuk LSTM dan Random Forest\n",
    "max_len = 150     # Panjang maksimum input untuk LSTM\n",
    "embedding_dim = 128  # Dimensionality of embedding untuk LSTM\n",
    "\n",
    "# Label Binarization untuk label yang sudah di-resample\n",
    "lb = LabelBinarizer()\n",
    "y_train_onehot = lb.fit_transform(y_train_resampled)\n",
    "\n",
    "# Flattening untuk kompatibilitas dengan model\n",
    "y_train_flat = y_train_onehot.argmax(axis=1)\n",
    "\n",
    "# Cek ukuran matriks resampled\n",
    "print(f\"Shape of resampled matrix: {X_train_resampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20deb890-4553-4c76-a0c3-b286acf71ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3796/3796 [==============================] - 97s 25ms/step - loss: 1.3339 - accuracy: 0.4174 - val_loss: 0.8698 - val_accuracy: 0.7532\n",
      "Epoch 2/100\n",
      "3796/3796 [==============================] - 91s 24ms/step - loss: 1.2075 - accuracy: 0.4624 - val_loss: 0.8142 - val_accuracy: 0.7087\n",
      "Epoch 3/100\n",
      "3796/3796 [==============================] - 93s 25ms/step - loss: 1.1837 - accuracy: 0.4749 - val_loss: 0.8017 - val_accuracy: 0.7734\n",
      "Epoch 4/100\n",
      "3796/3796 [==============================] - 96s 25ms/step - loss: 1.1692 - accuracy: 0.4811 - val_loss: 0.7860 - val_accuracy: 0.7722\n",
      "Epoch 5/100\n",
      "3796/3796 [==============================] - 104s 27ms/step - loss: 1.1570 - accuracy: 0.4883 - val_loss: 0.7687 - val_accuracy: 0.7978\n",
      "Epoch 6/100\n",
      "3796/3796 [==============================] - 100s 26ms/step - loss: 1.1495 - accuracy: 0.4922 - val_loss: 0.7316 - val_accuracy: 0.7994\n",
      "Epoch 7/100\n",
      "3796/3796 [==============================] - 103s 27ms/step - loss: 1.1408 - accuracy: 0.4971 - val_loss: 0.8058 - val_accuracy: 0.7190\n",
      "Epoch 8/100\n",
      "3796/3796 [==============================] - 97s 26ms/step - loss: 1.1341 - accuracy: 0.5003 - val_loss: 0.8184 - val_accuracy: 0.7669\n",
      "Epoch 9/100\n",
      "3796/3796 [==============================] - 95s 25ms/step - loss: 1.1266 - accuracy: 0.5050 - val_loss: 0.7926 - val_accuracy: 0.7907\n",
      "Epoch 10/100\n",
      "3796/3796 [==============================] - 99s 26ms/step - loss: 1.1208 - accuracy: 0.5075 - val_loss: 0.7612 - val_accuracy: 0.7741\n",
      "Epoch 11/100\n",
      "3796/3796 [==============================] - 97s 26ms/step - loss: 1.1144 - accuracy: 0.5117 - val_loss: 0.8574 - val_accuracy: 0.7644\n",
      "Epoch 12/100\n",
      "3796/3796 [==============================] - 98s 26ms/step - loss: 1.1088 - accuracy: 0.5141 - val_loss: 0.7905 - val_accuracy: 0.7924\n",
      "Epoch 13/100\n",
      "3796/3796 [==============================] - 96s 25ms/step - loss: 1.1021 - accuracy: 0.5185 - val_loss: 0.8319 - val_accuracy: 0.7694\n",
      "Epoch 14/100\n",
      "3796/3796 [==============================] - 101s 27ms/step - loss: 1.0971 - accuracy: 0.5200 - val_loss: 0.7700 - val_accuracy: 0.8018\n",
      "Epoch 15/100\n",
      "3796/3796 [==============================] - 95s 25ms/step - loss: 1.0950 - accuracy: 0.5228 - val_loss: 0.8273 - val_accuracy: 0.7816\n",
      "Epoch 16/100\n",
      "3796/3796 [==============================] - 96s 25ms/step - loss: 1.0860 - accuracy: 0.5266 - val_loss: 0.8922 - val_accuracy: 0.7025\n",
      "Epoch 17/100\n",
      "3796/3796 [==============================] - 98s 26ms/step - loss: 1.0825 - accuracy: 0.5286 - val_loss: 0.8192 - val_accuracy: 0.7943\n",
      "Epoch 18/100\n",
      "3796/3796 [==============================] - 99s 26ms/step - loss: 1.0769 - accuracy: 0.5312 - val_loss: 0.8700 - val_accuracy: 0.7437\n",
      "Epoch 19/100\n",
      "3796/3796 [==============================] - 99s 26ms/step - loss: 1.0736 - accuracy: 0.5342 - val_loss: 0.8235 - val_accuracy: 0.7878\n",
      "Epoch 20/100\n",
      "3796/3796 [==============================] - 103s 27ms/step - loss: 1.0678 - accuracy: 0.5363 - val_loss: 0.8737 - val_accuracy: 0.7682\n",
      "Epoch 21/100\n",
      "3796/3796 [==============================] - 102s 27ms/step - loss: 1.0632 - accuracy: 0.5386 - val_loss: 0.8882 - val_accuracy: 0.7503\n",
      "Epoch 22/100\n",
      "3796/3796 [==============================] - 100s 26ms/step - loss: 1.0596 - accuracy: 0.5406 - val_loss: 0.8498 - val_accuracy: 0.7775\n",
      "Epoch 23/100\n",
      "3796/3796 [==============================] - 100s 26ms/step - loss: 1.0562 - accuracy: 0.5428 - val_loss: 0.9085 - val_accuracy: 0.7381\n",
      "Epoch 24/100\n",
      "3796/3796 [==============================] - 112s 30ms/step - loss: 1.0530 - accuracy: 0.5434 - val_loss: 0.8829 - val_accuracy: 0.7779\n",
      "618/618 [==============================] - 9s 14ms/step - loss: 0.7700 - accuracy: 0.8018\n",
      "618/618 [==============================] - 9s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "# --- Define and Train the LSTM Model ---\n",
    "lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(max_words, embedding_dim, input_length=max_len),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.01))),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(y_train_onehot.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "lstm_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the LSTM model\n",
    "lstm_model.fit(X_train_resampled, y_train_onehot_resampled, epochs=100, batch_size=64,\n",
    "               validation_data=(X_test_padded, y_test_onehot), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the LSTM model\n",
    "lstm_test_loss, lstm_test_accuracy = lstm_model.evaluate(X_test_padded, y_test_onehot)\n",
    "\n",
    "# Predict with the LSTM model\n",
    "y_pred_lstm = lstm_model.predict(X_test_padded).argmax(axis=1)\n",
    "\n",
    "# LSTM metrics\n",
    "lstm_precision = precision_score(y_test_flat, y_pred_lstm, average='weighted')\n",
    "lstm_recall = recall_score(y_test_flat, y_pred_lstm, average='weighted')\n",
    "lstm_f1 = f1_score(y_test_flat, y_pred_lstm, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b01febf2-8d23-4e8a-bbb7-3d103b9d48bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Random Forest Model ---\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_resampled, y_train_flat)\n",
    "\n",
    "# Predict with Random Forest\n",
    "y_pred_rf = rf_model.predict(X_test_padded)\n",
    "\n",
    "# Random Forest metrics\n",
    "rf_accuracy = accuracy_score(y_test_flat, y_pred_rf)\n",
    "rf_precision = precision_score(y_test_flat, y_pred_rf, average='weighted')\n",
    "rf_recall = recall_score(y_test_flat, y_pred_rf, average='weighted')\n",
    "rf_f1 = f1_score(y_test_flat, y_pred_rf, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d48fd1af-a409-4cef-aa59-b0f43c7b094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- XGBoost Model ---\n",
    "xgb_model = XGBClassifier(eval_metric='mlogloss')\n",
    "xgb_model.fit(X_train_resampled, y_train_flat)\n",
    "\n",
    "# Predict with XGBoost\n",
    "y_pred_xgb = xgb_model.predict(X_test_padded)\n",
    "\n",
    "# XGBoost metrics\n",
    "xgb_accuracy = accuracy_score(y_test_flat, y_pred_xgb)\n",
    "xgb_precision = precision_score(y_test_flat, y_pred_xgb, average='weighted')\n",
    "xgb_recall = recall_score(y_test_flat, y_pred_xgb, average='weighted')\n",
    "xgb_f1 = f1_score(y_test_flat, y_pred_xgb, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c2cdec4-74b0-4493-bc2c-8d6f4ed3cd0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to MultinomialNB (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train Naive Bayes model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m nb_model \u001b[38;5;241m=\u001b[39m MultinomialNB(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)  \u001b[38;5;66;03m# Adjust alpha for smoothing\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mnb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m      6\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m nb_model\u001b[38;5;241m.\u001b[39mpredict(X_test_padded)\n",
      "File \u001b[1;32m~\\PycharmProjects\\mobile_jkn_baru\\.venv\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\PycharmProjects\\mobile_jkn_baru\\.venv\\lib\\site-packages\\sklearn\\naive_bayes.py:759\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    757\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_counters(n_classes, n_features)\n\u001b[1;32m--> 759\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_alpha()\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_feature_log_prob(alpha)\n",
      "File \u001b[1;32m~\\PycharmProjects\\mobile_jkn_baru\\.venv\\lib\\site-packages\\sklearn\\naive_bayes.py:881\u001b[0m, in \u001b[0;36mMultinomialNB._count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_count\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y):\n\u001b[0;32m    880\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 881\u001b[0m     \u001b[43mcheck_non_negative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMultinomialNB (input X)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    882\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m safe_sparse_dot(Y\u001b[38;5;241m.\u001b[39mT, X)\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\PycharmProjects\\mobile_jkn_baru\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:1689\u001b[0m, in \u001b[0;36mcheck_non_negative\u001b[1;34m(X, whom)\u001b[0m\n\u001b[0;32m   1686\u001b[0m     X_min \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmin(X)\n\u001b[0;32m   1688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_min \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1689\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNegative values in data passed to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m whom)\n",
      "\u001b[1;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
     ]
    }
   ],
   "source": [
    "# Train Naive Bayes model\n",
    "nb_model = MultinomialNB(alpha=0.001)  # Adjust alpha for smoothing\n",
    "nb_model.fit(X_train_resampled, y_train_flat)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = nb_model.predict(X_test_padded)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_flat, y_pred)\n",
    "precision = precision_score(y_test_flat, y_pred, average='weighted')\n",
    "recall = recall_score(y_test_flat, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test_flat, y_pred, average='weighted')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa63f5a2-b2c8-4bed-b482-f8cb36cd4164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of Model Results With SMOTE:\n",
      "           Model  Accuracy  Precision    Recall  F1-Score\n",
      "0           LSTM  0.801792   0.772197  0.801792  0.783995\n",
      "1  Random Forest  0.701549   0.717198  0.701549  0.702007\n",
      "2        XGBoost  0.705954   0.746207  0.705954  0.724851\n",
      "3    Naive Bayes  0.609508   0.650668  0.609508  0.569608\n"
     ]
    }
   ],
   "source": [
    "# --- Model Results ---\n",
    "results = {\n",
    "    \"Model\": [\"LSTM\", \"Random Forest\", \"XGBoost\", \"Naive Bayes\"],\n",
    "    \"Accuracy\": [lstm_test_accuracy, rf_accuracy, xgb_accuracy, accuracy],\n",
    "    \"Precision\": [lstm_precision, rf_precision, xgb_precision, precision],\n",
    "    \"Recall\": [lstm_recall, rf_recall, xgb_recall, recall],\n",
    "    \"F1-Score\": [lstm_f1, rf_f1, xgb_f1, f1]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the DataFrame as a table\n",
    "print(\"\\nComparison of Model Results With SMOTE:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca712f2-df85-4edc-a879-f38e0065110b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
